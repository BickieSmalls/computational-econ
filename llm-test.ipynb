{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/computational-econ/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe cuda memory\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-1b-redpajama-200b-dolly/d3586068c3d023c7fcfa3c7dbd3042b2f00db1e3/attention.py:281: UserWarning: While `attn_impl: triton` can be faster than `attn_impl: flash` it uses more memory. When training larger models this can trigger alloc retries which hurts performance. If encountered, we recommend using `attn_impl: flash` if your model does not use `alibi` or `prefix_lm`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cuda:0', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MosaicGPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50432, 2048)\n",
       "    (emb_drop): Dropout(p=0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (1): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (2): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (3): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (4): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (5): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (6): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (7): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (8): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (9): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (10): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (11): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (12): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (13): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (14): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (15): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (16): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (17): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (18): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (19): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (20): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (21): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (22): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (23): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the model is available locally first\n",
    "name = 'mpt-1b-redpajama-200b-dolly'\n",
    "\n",
    "if not os.path.exists(name):\n",
    "    print(f'Downloading {name} from the hub')\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        f'mosaicml/{name}',\n",
    "        trust_remote_code=True, \n",
    "        torch_dtype=torch.float16\n",
    "        )\n",
    "    # save the model to disk locally\n",
    "    model.save_pretrained(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-1b-redpajama-200b-dolly/d3586068c3d023c7fcfa3c7dbd3042b2f00db1e3/attention.py:281: UserWarning: While `attn_impl: triton` can be faster than `attn_impl: flash` it uses more memory. When training larger models this can trigger alloc retries which hurts performance. If encountered, we recommend using `attn_impl: flash` if your model does not use `alibi` or `prefix_lm`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cuda:0', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MosaicGPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50432, 2048)\n",
       "    (emb_drop): Dropout(p=0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (1): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (2): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (3): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (4): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (5): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (6): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (7): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (8): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (9): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (10): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (11): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (12): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (13): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (14): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (15): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (16): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (17): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (18): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (19): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (20): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (21): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (22): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (23): GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\n",
    "config.init_device = 'cuda:0' # For fast initialization directly on GPU! \n",
    "# load the model from disk to cuda\n",
    "# convert the model to fp16\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    name,\n",
    "    config=config,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    "    ).half()\n",
    "\n",
    "model.to(device='cuda:0', dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# check if triton is enabled in model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39mattn_impl)\n\u001b[1;32m      4\u001b[0m \u001b[39m# change the attention implementation to default\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mattn_impl \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "# check if triton is enabled in model\n",
    "print(model.config.attn_impl)\n",
    "\n",
    "# change the attention implementation to default\n",
    "model.config.attn_impl = 'default'\n",
    "print(model.config.attn_impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "inputs = tokenizer([\"Tell me about Obama\"], return_tensors=\"pt\")\n",
    "inputs=inputs.to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    2502 MB |    2700 MB |    5005 MB |    2503 MB |\n",
      "|       from large pool |    2502 MB |    2700 MB |    5004 MB |    2502 MB |\n",
      "|       from small pool |       0 MB |       1 MB |       1 MB |       1 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    2502 MB |    2700 MB |    5005 MB |    2503 MB |\n",
      "|       from large pool |    2502 MB |    2700 MB |    5004 MB |    2502 MB |\n",
      "|       from small pool |       0 MB |       1 MB |       1 MB |       1 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    2750 MB |    2750 MB |    2750 MB |       0 B  |\n",
      "|       from large pool |    2748 MB |    2748 MB |    2748 MB |       0 B  |\n",
      "|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  196219 KB |  253564 KB |  550500 KB |  354281 KB |\n",
      "|       from large pool |  194560 KB |  251904 KB |  546816 KB |  352256 KB |\n",
      "|       from small pool |    1659 KB |    2044 KB |    3684 KB |    2025 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     196    |     387    |     583    |     387    |\n",
      "|       from large pool |      97    |      98    |     194    |      97    |\n",
      "|       from small pool |      99    |     290    |     389    |     290    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     196    |     387    |     583    |     387    |\n",
      "|       from large pool |      97    |      98    |     194    |      97    |\n",
      "|       from small pool |      99    |     290    |     389    |     290    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      87    |      87    |      87    |       0    |\n",
      "|       from large pool |      86    |      86    |      86    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      40    |     109    |     143    |     103    |\n",
      "|       from large pool |      14    |      16    |      27    |      13    |\n",
      "|       from small pool |      26    |      97    |     116    |      90    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list the objects in cuda memory\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[17570,   479,   670,  6729]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.2559]]], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize random tensor on cuda\n",
    "x = torch.randn(1, 1, 1, device='cuda:0')\n",
    "\n",
    "# create a 1 layer model nn.Linear\n",
    "model1 = torch.nn.Linear(1, 1).to(device='cuda:0')\n",
    "\n",
    "# run the model on the random tensor\n",
    "model1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA: Error- no device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<string>:21\u001b[0m, in \u001b[0;36m_fwd_kernel\u001b[0;34m(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob, stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE, IS_CAUSAL, BLOCK_HEADDIM, EVEN_M, EVEN_N, EVEN_HEADDIM, BLOCK_M, BLOCK_N, grid, num_warps, num_stages, extern_libs, stream, warmup)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('2-.-0-.-0-3d28bf8dd4111863b189d74cf84b730b-2b0c5161c53c71b37ae20a9996ee4bb8-c1f92808b4e4644c1732e8338187ac87-d962222789c30252d492a16cca3bf467-12f7ac1ca211e037f62a7c0c323d9990-5c5e32ff210f3b7f56c98ca29917c25e-06f0df2d61979d629033f4a22eff5198-0dd03b0bd512a184b3512b278d9dfa59-d35ab04ae841e2714a253c523530b071', (torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.float32, torch.float32, 'fp32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32'), ('vector', True, 128, False, False, True, 128, 128), (True, True, True, True, True, True, True, (False,), (True, False), (True, False), (True, False), (True, False), (True, False), (True, False), (True, False), (True, False), (True, False), (True, False), (False, False), (True, False), (True, False), (True, False), (True, False), (True, False), (False, False), (False, False), (True, False), (True, False), (True, False), (True, False)))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# generate text using the model on cuda\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m      3\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \n\u001b[1;32m      4\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, \n\u001b[1;32m      5\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m      6\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m, \n\u001b[1;32m      7\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m)\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/transformers/generation/utils.py:1572\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1564\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1565\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1566\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1567\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1568\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1569\u001b[0m     )\n\u001b[1;32m   1571\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1573\u001b[0m         input_ids,\n\u001b[1;32m   1574\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1575\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1576\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1577\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1578\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1579\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1580\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1581\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1582\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1583\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1584\u001b[0m     )\n\u001b[1;32m   1586\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1587\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/transformers/generation/utils.py:2619\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2618\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2619\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2620\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2621\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2622\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2623\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2624\u001b[0m )\n\u001b[1;32m   2626\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2627\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-1b-redpajama-200b-dolly/d3586068c3d023c7fcfa3c7dbd3042b2f00db1e3/mosaic_gpt.py:351\u001b[0m, in \u001b[0;36mMosaicGPT.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, prefix_mask, sequence_id, return_dict, output_attentions, output_hidden_states, use_cache)\u001b[0m\n\u001b[1;32m    348\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (x,)\n\u001b[1;32m    349\u001b[0m past_key_value \u001b[39m=\u001b[39m past_key_values[\n\u001b[1;32m    350\u001b[0m     b_idx] \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m x, past_key_value \u001b[39m=\u001b[39m block(x,\n\u001b[1;32m    352\u001b[0m                           past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    353\u001b[0m                           attn_bias\u001b[39m=\u001b[39;49mattn_bias,\n\u001b[1;32m    354\u001b[0m                           attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    355\u001b[0m                           is_causal\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_causal)\n\u001b[1;32m    356\u001b[0m \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     past_key_values[b_idx] \u001b[39m=\u001b[39m past_key_value\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-1b-redpajama-200b-dolly/d3586068c3d023c7fcfa3c7dbd3042b2f00db1e3/gpt_blocks.py:81\u001b[0m, in \u001b[0;36mGPTBlock.forward\u001b[0;34m(self, x, past_key_value, attn_bias, attention_mask, is_causal)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     73\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     74\u001b[0m     x: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     is_causal: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     79\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, Optional[Tuple[torch\u001b[39m.\u001b[39mTensor]]]:\n\u001b[1;32m     80\u001b[0m     a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(x)\n\u001b[0;32m---> 81\u001b[0m     b, _, past_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(a,\n\u001b[1;32m     82\u001b[0m                                      past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m     83\u001b[0m                                      attn_bias\u001b[39m=\u001b[39;49mattn_bias,\n\u001b[1;32m     84\u001b[0m                                      attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     85\u001b[0m                                      is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m     86\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_attn_dropout(b)\n\u001b[1;32m     87\u001b[0m     m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x)\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-1b-redpajama-200b-dolly/d3586068c3d023c7fcfa3c7dbd3042b2f00db1e3/attention.py:332\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, x, past_key_value, attn_bias, attention_mask, is_causal, needs_weights)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m attn_bias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     attn_bias \u001b[39m=\u001b[39m attn_bias[:, :, \u001b[39m-\u001b[39mquery\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m):, \u001b[39m-\u001b[39mkey\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m):]\n\u001b[0;32m--> 332\u001b[0m context, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_fn(\n\u001b[1;32m    333\u001b[0m     query,\n\u001b[1;32m    334\u001b[0m     key,\n\u001b[1;32m    335\u001b[0m     value,\n\u001b[1;32m    336\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_heads,\n\u001b[1;32m    337\u001b[0m     softmax_scale\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax_scale,\n\u001b[1;32m    338\u001b[0m     attn_bias\u001b[39m=\u001b[39;49mattn_bias,\n\u001b[1;32m    339\u001b[0m     key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    340\u001b[0m     is_causal\u001b[39m=\u001b[39;49mis_causal,\n\u001b[1;32m    341\u001b[0m     dropout_p\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_dropout_p,\n\u001b[1;32m    342\u001b[0m     training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m    343\u001b[0m     needs_weights\u001b[39m=\u001b[39;49mneeds_weights,\n\u001b[1;32m    344\u001b[0m )\n\u001b[1;32m    346\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_proj(context), attn_weights, past_key_value\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-1b-redpajama-200b-dolly/d3586068c3d023c7fcfa3c7dbd3042b2f00db1e3/attention.py:226\u001b[0m, in \u001b[0;36mtriton_flash_attn_fn\u001b[0;34m(query, key, value, n_heads, softmax_scale, attn_bias, key_padding_mask, is_causal, dropout_p, training, needs_weights)\u001b[0m\n\u001b[1;32m    223\u001b[0m value \u001b[39m=\u001b[39m rearrange(value, \u001b[39m'\u001b[39m\u001b[39mb s (h d) -> b s h d\u001b[39m\u001b[39m'\u001b[39m, h\u001b[39m=\u001b[39mn_heads)\n\u001b[1;32m    225\u001b[0m reset_is_causal \u001b[39m=\u001b[39m _reset_is_causal(query\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), key\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), is_causal)\n\u001b[0;32m--> 226\u001b[0m attn_output \u001b[39m=\u001b[39m flash_attn_triton\u001b[39m.\u001b[39;49mflash_attn_func(query, key, value,\n\u001b[1;32m    227\u001b[0m                                                 attn_bias, reset_is_causal,\n\u001b[1;32m    228\u001b[0m                                                 softmax_scale)\n\u001b[1;32m    230\u001b[0m output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mview(\u001b[39m*\u001b[39mattn_output\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m output, \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/flash_attn/flash_attn_triton.py:804\u001b[0m, in \u001b[0;36mFlashAttnFunc.forward\u001b[0;34m(ctx, q, k, v, bias, causal, softmax_scale)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[39m# Make sure that the last dimension is contiguous\u001b[39;00m\n\u001b[1;32m    803\u001b[0m q, k, v \u001b[39m=\u001b[39m [x \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mstride(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m x\u001b[39m.\u001b[39mcontiguous() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m [q, k, v]]\n\u001b[0;32m--> 804\u001b[0m o, lse, ctx\u001b[39m.\u001b[39msoftmax_scale \u001b[39m=\u001b[39m _flash_attn_forward(\n\u001b[1;32m    805\u001b[0m     q, k, v, bias\u001b[39m=\u001b[39;49mbias, causal\u001b[39m=\u001b[39;49mcausal, softmax_scale\u001b[39m=\u001b[39;49msoftmax_scale\n\u001b[1;32m    806\u001b[0m )\n\u001b[1;32m    807\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(q, k, v, o, lse, bias)\n\u001b[1;32m    808\u001b[0m ctx\u001b[39m.\u001b[39mcausal \u001b[39m=\u001b[39m causal\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/flash_attn/flash_attn_triton.py:618\u001b[0m, in \u001b[0;36m_flash_attn_forward\u001b[0;34m(q, k, v, bias, causal, softmax_scale)\u001b[0m\n\u001b[1;32m    616\u001b[0m num_warps \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m \u001b[39mif\u001b[39;00m d \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m64\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m8\u001b[39m\n\u001b[1;32m    617\u001b[0m grid \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m META: (triton\u001b[39m.\u001b[39mcdiv(seqlen_q, META[\u001b[39m\"\u001b[39m\u001b[39mBLOCK_M\u001b[39m\u001b[39m\"\u001b[39m]), batch \u001b[39m*\u001b[39m nheads)\n\u001b[0;32m--> 618\u001b[0m _fwd_kernel[grid](\n\u001b[1;32m    619\u001b[0m     q, k, v, bias, o,\n\u001b[1;32m    620\u001b[0m     lse, tmp,\n\u001b[1;32m    621\u001b[0m     softmax_scale,\n\u001b[1;32m    622\u001b[0m     q\u001b[39m.\u001b[39;49mstride(\u001b[39m0\u001b[39;49m), q\u001b[39m.\u001b[39;49mstride(\u001b[39m2\u001b[39;49m), q\u001b[39m.\u001b[39;49mstride(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    623\u001b[0m     k\u001b[39m.\u001b[39;49mstride(\u001b[39m0\u001b[39;49m), k\u001b[39m.\u001b[39;49mstride(\u001b[39m2\u001b[39;49m), k\u001b[39m.\u001b[39;49mstride(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    624\u001b[0m     v\u001b[39m.\u001b[39;49mstride(\u001b[39m0\u001b[39;49m), v\u001b[39m.\u001b[39;49mstride(\u001b[39m2\u001b[39;49m), v\u001b[39m.\u001b[39;49mstride(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    625\u001b[0m     \u001b[39m*\u001b[39;49mbias_strides,\n\u001b[1;32m    626\u001b[0m     o\u001b[39m.\u001b[39;49mstride(\u001b[39m0\u001b[39;49m), o\u001b[39m.\u001b[39;49mstride(\u001b[39m2\u001b[39;49m), o\u001b[39m.\u001b[39;49mstride(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    627\u001b[0m     nheads, seqlen_q, seqlen_k, seqlen_q_rounded, d,\n\u001b[1;32m    628\u001b[0m     seqlen_q \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m32\u001b[39;49m,  seqlen_k \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m32\u001b[39;49m, \u001b[39m# key for triton cache (limit number of compilations)\u001b[39;49;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39m# Can't use kwargs here because triton autotune expects key to be args, not kwargs\u001b[39;49;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39m# IS_CAUSAL=causal, BLOCK_HEADDIM=d,\u001b[39;49;00m\n\u001b[1;32m    631\u001b[0m     bias_type, causal, BLOCK_HEADDIM,\n\u001b[1;32m    632\u001b[0m     BLOCK_M\u001b[39m=\u001b[39;49mBLOCK, BLOCK_N\u001b[39m=\u001b[39;49mBLOCK,\n\u001b[1;32m    633\u001b[0m     num_warps\u001b[39m=\u001b[39;49mnum_warps,\n\u001b[1;32m    634\u001b[0m     num_stages\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    635\u001b[0m )\n\u001b[1;32m    636\u001b[0m \u001b[39mreturn\u001b[39;00m o, lse, softmax_scale\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/triton/runtime/jit.py:106\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.launcher\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlauncher\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39m*\u001b[39;49margs, grid\u001b[39m=\u001b[39;49mgrid, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/triton/runtime/autotuner.py:200\u001b[0m, in \u001b[0;36mHeuristics.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mfor\u001b[39;00m v, heur \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    199\u001b[0m     kwargs[v] \u001b[39m=\u001b[39m heur({\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg_names, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs})\n\u001b[0;32m--> 200\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn\u001b[39m.\u001b[39;49mrun(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m<string>:41\u001b[0m, in \u001b[0;36m_fwd_kernel\u001b[0;34m(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob, stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE, IS_CAUSAL, BLOCK_HEADDIM, EVEN_M, EVEN_N, EVEN_HEADDIM, BLOCK_M, BLOCK_N, grid, num_warps, num_stages, extern_libs, stream, warmup)\u001b[0m\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/triton/compiler.py:1256\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(fn, signature, device, constants, num_warps, num_stages, extern_libs, configs, cc, warm_cache_only)\u001b[0m\n\u001b[1;32m   1250\u001b[0m llir_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.llir\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fn_cache_manager\u001b[39m.\u001b[39mhas_file(cubin_name) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m   1252\u001b[0m    \u001b[39mnot\u001b[39;00m fn_cache_manager\u001b[39m.\u001b[39mhas_file(data_name) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m   1253\u001b[0m    \u001b[39mnot\u001b[39;00m fn_cache_manager\u001b[39m.\u001b[39mhas_file(ptx_name) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m   1254\u001b[0m    \u001b[39mnot\u001b[39;00m fn_cache_manager\u001b[39m.\u001b[39mhas_file(ttir_name) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m   1255\u001b[0m    \u001b[39mnot\u001b[39;00m fn_cache_manager\u001b[39m.\u001b[39mhas_file(llir_name):\n\u001b[0;32m-> 1256\u001b[0m     asm, shared, kernel_name \u001b[39m=\u001b[39m _compile(fn, signature, device, constants, configs[\u001b[39m0\u001b[39;49m], num_warps, num_stages,\n\u001b[1;32m   1257\u001b[0m                                         extern_libs, \u001b[39m\"\u001b[39;49m\u001b[39mcubin\u001b[39;49m\u001b[39m\"\u001b[39;49m, cc)\n\u001b[1;32m   1258\u001b[0m     metadata \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: kernel_name, \u001b[39m\"\u001b[39m\u001b[39mshared\u001b[39m\u001b[39m\"\u001b[39m: shared, \u001b[39m\"\u001b[39m\u001b[39mnum_warps\u001b[39m\u001b[39m\"\u001b[39m: num_warps, \u001b[39m\"\u001b[39m\u001b[39mnum_stages\u001b[39m\u001b[39m\"\u001b[39m: num_stages}\n\u001b[1;32m   1259\u001b[0m     fn_cache_manager\u001b[39m.\u001b[39mput(asm[\u001b[39m\"\u001b[39m\u001b[39mcubin\u001b[39m\u001b[39m\"\u001b[39m], cubin_name)\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/triton/compiler.py:901\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(fn, signature, device, constants, specialization, num_warps, num_stages, extern_libs, output, cc)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[39mif\u001b[39;00m extern_libs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    900\u001b[0m     extern_libs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m--> 901\u001b[0m name, asm, shared_mem \u001b[39m=\u001b[39m _triton\u001b[39m.\u001b[39;49mcode_gen\u001b[39m.\u001b[39;49mcompile_ttir(backend, module, device, num_warps, num_stages, extern_libs, cc)\n\u001b[1;32m    902\u001b[0m \u001b[39mreturn\u001b[39;00m asm, shared_mem, name\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA: Error- no device"
     ]
    }
   ],
   "source": [
    "# generate text using the model on cuda\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    max_length=100, \n",
    "    do_sample=True, \n",
    "    top_p=0.95, \n",
    "    top_k=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating an MPTForCausalLM model from /home/nathan/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/a5e85ae1941e31bb705adbcafce9b0dfd6f3a48b/modeling_mpt.py\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 11.00 GiB total capacity; 7.26 GiB already allocated; 2.33 GiB free; 7.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m config\u001b[39m.\u001b[39minit_device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m \u001b[39m# For fast initialization directly on GPU!\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# batch size of 16\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mAutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      9\u001b[0m   name,\n\u001b[1;32m     10\u001b[0m   config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     11\u001b[0m   torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16, \u001b[39m# Load model weights in bfloat16\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m   trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:479\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m     model_class \u001b[39m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m    476\u001b[0m         class_ref, pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    477\u001b[0m     )\n\u001b[1;32m    478\u001b[0m     _ \u001b[39m=\u001b[39m hub_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcode_revision\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 479\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    480\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    481\u001b[0m     )\n\u001b[1;32m    482\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    483\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/transformers/modeling_utils.py:2675\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2672\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2674\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2675\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2677\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2678\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/a5e85ae1941e31bb705adbcafce9b0dfd6f3a48b/modeling_mpt.py:231\u001b[0m, in \u001b[0;36mMPTForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mMPTForCausalLM only supports tied word embeddings\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    230\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInstantiating an MPTForCausalLM model from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m__file__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 231\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m MPTModel(config)\n\u001b[1;32m    232\u001b[0m \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(child, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModuleList):\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/a5e85ae1941e31bb705adbcafce9b0dfd6f3a48b/modeling_mpt.py:57\u001b[0m, in \u001b[0;36mMPTModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_seq_len, config\u001b[39m.\u001b[39md_model, device\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39minit_device)\n\u001b[1;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_drop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39memb_pdrop)\n\u001b[0;32m---> 57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([MPTBlock(device\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39minit_device, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig\u001b[39m.\u001b[39mto_dict()) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mn_layers)])\n\u001b[1;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_f \u001b[39m=\u001b[39m norm_class(config\u001b[39m.\u001b[39md_model, device\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39minit_device)\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39minit_device \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/a5e85ae1941e31bb705adbcafce9b0dfd6f3a48b/modeling_mpt.py:57\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_seq_len, config\u001b[39m.\u001b[39md_model, device\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39minit_device)\n\u001b[1;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_drop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39memb_pdrop)\n\u001b[0;32m---> 57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([MPTBlock(device\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49minit_device, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig\u001b[39m.\u001b[39;49mto_dict()) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mn_layers)])\n\u001b[1;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_f \u001b[39m=\u001b[39m norm_class(config\u001b[39m.\u001b[39md_model, device\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39minit_device)\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39minit_device \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/a5e85ae1941e31bb705adbcafce9b0dfd6f3a48b/blocks.py:30\u001b[0m, in \u001b[0;36mMPTBlock.__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn \u001b[39m=\u001b[39m attn_class(attn_impl\u001b[39m=\u001b[39mattn_config[\u001b[39m'\u001b[39m\u001b[39mattn_impl\u001b[39m\u001b[39m'\u001b[39m], clip_qkv\u001b[39m=\u001b[39mattn_config[\u001b[39m'\u001b[39m\u001b[39mclip_qkv\u001b[39m\u001b[39m'\u001b[39m], qk_ln\u001b[39m=\u001b[39mattn_config[\u001b[39m'\u001b[39m\u001b[39mqk_ln\u001b[39m\u001b[39m'\u001b[39m], softmax_scale\u001b[39m=\u001b[39mattn_config[\u001b[39m'\u001b[39m\u001b[39msoftmax_scale\u001b[39m\u001b[39m'\u001b[39m], attn_pdrop\u001b[39m=\u001b[39mattn_config[\u001b[39m'\u001b[39m\u001b[39mattn_pdrop\u001b[39m\u001b[39m'\u001b[39m], d_model\u001b[39m=\u001b[39md_model, n_heads\u001b[39m=\u001b[39mn_heads, verbose\u001b[39m=\u001b[39mverbose, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_2 \u001b[39m=\u001b[39m norm_class(d_model, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> 30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn \u001b[39m=\u001b[39m MPTMLP(d_model\u001b[39m=\u001b[39;49md_model, expansion_ratio\u001b[39m=\u001b[39;49mexpansion_ratio, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_attn_dropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(resid_pdrop)\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_ffn_dropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(resid_pdrop)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/a5e85ae1941e31bb705adbcafce9b0dfd6f3a48b/blocks.py:12\u001b[0m, in \u001b[0;36mMPTMLP.__init__\u001b[0;34m(self, d_model, expansion_ratio, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, d_model: \u001b[39mint\u001b[39m, expansion_ratio: \u001b[39mint\u001b[39m, device: Optional[\u001b[39mstr\u001b[39m]\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     11\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(d_model, expansion_ratio \u001b[39m*\u001b[39;49m d_model, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mGELU(approximate\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(expansion_ratio \u001b[39m*\u001b[39m d_model, d_model, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/computational-econ/lib/python3.10/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 11.00 GiB total capacity; 7.26 GiB already allocated; 2.33 GiB free; 7.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# name = 'mosaicml/mpt-7b-storywriter'\n",
    "\n",
    "# config = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\n",
    "# config.attn_config['attn_impl'] = 'triton'\n",
    "# config.init_device = 'cuda:0' # For fast initialization directly on GPU!\n",
    "\n",
    "# # batch size of 16\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#   name,\n",
    "#   config=config,\n",
    "#   torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n",
    "#   trust_remote_code=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-econ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
